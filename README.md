# ModulaRL
<div align="center">
  <img src="assets/modulaRL_logo.svg" alt="ModulaRL Logo">
</div>

<div align="center">
  ðŸš§ This library is still under construction. ðŸš§
</div>

[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![pytest](https://img.shields.io/badge/tested%20with-pytest-46a2f1.svg)](https://docs.pytest.org/en/stable/)

ModulaRL is a highly modular and extensible reinforcement learning library built on PyTorch. It aims to provide researchers and developers with a flexible framework for implementing, experimenting with, and extending various RL algorithms.

## Features

- Modular architecture allowing easy component swapping and extension
- Efficient implementations leveraging PyTorch's capabilities
- Integration with TorchRL for optimized replay buffers
- Clear documentation and examples for quick start
- Designed for both research and practical applications in reinforcement learning

## Installation

```bash
pip install modularl
```
## Algorithms Implemented

| Algorithm | Type | Paper |
|-----------|------|-------|
| SAC (Soft Actor-Critic) | Off-policy | [Haarnoja et al. 2018](https://arxiv.org/abs/1801.01290) |


## Citation
```
@software{modularl2024,
  author = {zakaria narjis},
  title = {ModulaRL: A Modular Reinforcement Learning Library},
  year = {2024},
  url = {https://github.com/zakaria-narjis/modularl}
}
```
